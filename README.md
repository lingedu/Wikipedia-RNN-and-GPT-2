# Wikipedia-RNN-and-GPT-2

In this repository, we generate Wikipedia by two different - GPT2 and LSTM. We let both models read the [Wikipedia](https://www.tensorflow.org/datasets/catalog/wikipedia) datasets and then generate the Wikipwdia-like text. We can modify diffent temporature to present diffent writing style - more normal or more creatrive. Because of the limitation on RAM, the articles we fit into the model is around 10000 to 20000, which might limit the model's ability to generate human-being-like text. If we increase the raw data, the result will be more useful.

_Inspired by_: [Train a GPT-2 Text-Generating Model w/ GPU](https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce) 
[text_generation_wikipedia_rnn](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/text_generation_wikipedia_rnn/text_generation_wikipedia_rnn.ipynb)

![](https://upload.wikimedia.org/wikipedia/commons/thumb/d/de/Wikipedia-logo_%28inverse%29.png/657px-Wikipedia-logo_%28inverse%29.png)
Credits: https://zh.wikipedia.org/wiki/File:Wikipedia-logo_(inverse).png

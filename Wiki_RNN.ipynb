{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wiki RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1gB2QnFJuHB-naJSGZQR0zNyAMcG9eD0N",
      "authorship_tag": "ABX9TyNpqIrIJNZq8ocdFgNmssUF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s99436q/Wikipedia-RNN-and-GPT-2/blob/master/Wiki_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNL3Ch67HUbH",
        "colab_type": "text"
      },
      "source": [
        "#Prepare data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "105jqjQy85F3",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "f8e8c0bd-791b-4386-8ef0-913a2fdefdb3"
      },
      "source": [
        "#@title set up { form-width: \"20%\" }\n",
        "#%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import platform\n",
        "import time\n",
        "import pathlib\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/GPT-2')\n",
        "#print('python version:',platform.python_version())\n",
        "#print('tensorflow version:',tf.__version__)\n",
        "#print(f'kears version: {tf.keras.__version__}')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1dIInZf-rv3",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title load data { output-height: 2, form-width: \"20%\" }\n",
        "dataset, ds_info = tfds.load(\n",
        "    name='wikipedia/20190301.en',\n",
        "    data_dir='gs://tfds-data/datasets',\n",
        "    with_info=True,\n",
        "    split=tfds.Split.TRAIN,\n",
        ")\n",
        "#print(ds_info)\n",
        "#print(ds_info.splits['train'].num_examples)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9_N6PYFHwyf",
        "colab_type": "text"
      },
      "source": [
        "#Preprocess the dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swqGCzFUHTq5",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "ded6f698-d4ca-4a4d-a058-927fab8a2068"
      },
      "source": [
        "#@title Flatten the dataset { output-height: 20, form-width: \"20%\" }\n",
        "#coverting the datasetfrom the set of articles to set of characters.\n",
        "def article_to_text(text):\n",
        "  return np.array([char for char in text.numpy().decode('utf-8')])\n",
        "\n",
        "dataset_text = dataset.map(\n",
        "    lambda x: tf.py_function(func=article_to_text, inp=[x['text']], Tout=tf.string))\n",
        "\n",
        "\n",
        "for x in dataset_text.take(2): \n",
        "    print(len(x),x.numpy())\n",
        "\n",
        "#covert the text dataset to more regular char-based dataset\n",
        "dataset_chars=dataset_text.unbatch()\n",
        "for x in dataset_chars.take(2): \n",
        "    print(x)#.numpy())#.decode('utf-8'))\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17851 [b'J' b'o' b's' ... b'n' b't' b's']\n",
            "2554 [b'P' b'a' b'u' ... b'e' b'r' b's']\n",
            "tf.Tensor(b'J', shape=(), dtype=string)\n",
            "tf.Tensor(b'o', shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1dxOWHmNgug",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "1dbfeb9f-1883-4f17-e707-382c6c0d366c"
      },
      "source": [
        "#@title generating vocabulary { output-height: 20, form-width: \"20%\" }\n",
        "vocab=set()\n",
        "for text in dataset_text.take(1000): #ideally we should take all dataset into\n",
        "    vocab.update([char.decode('utf-8') for char in text.numpy()])\n",
        "   \n",
        "vocab=sorted(vocab)\n",
        "print(f'num of unique vocab:{len(vocab)}\\n vocab set:{vocab}')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num of unique vocab:621\n",
            " vocab set:['\\t', '\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\xa0', '£', '§', '«', '®', '°', '±', '²', '·', '»', '¼', '½', '¿', 'Á', 'Å', 'Æ', 'Ç', 'É', 'Ë', 'Í', 'Î', 'Ó', 'Ö', '×', 'Ø', 'Ü', 'Þ', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ö', 'ø', 'ú', 'û', 'ü', 'ý', 'ā', 'ă', 'ą', 'Ć', 'ć', 'Č', 'č', 'đ', 'ė', 'ę', 'ě', 'ğ', 'ġ', 'Ħ', 'ī', 'İ', 'ı', 'ļ', 'Ł', 'ł', 'ń', 'ň', 'Ō', 'ō', 'ő', 'ř', 'Ś', 'ś', 'Ş', 'ş', 'Š', 'š', 'ţ', 'ū', 'ź', 'ż', 'Ž', 'ž', 'ơ', 'ư', 'ǔ', 'ș', 'ț', 'ɔ', 'ə', 'ɛ', 'ʷ', 'ʼ', 'ʿ', '˚', 'Ι', 'Π', 'α', 'β', 'ε', 'η', 'ι', 'κ', 'μ', 'ο', 'ρ', 'ς', 'τ', 'υ', 'χ', 'ψ', 'ό', 'Б', 'В', 'Д', 'Ж', 'З', 'И', 'К', 'Л', 'М', 'Н', 'О', 'П', 'С', 'У', 'Ф', 'Х', 'а', 'б', 'в', 'г', 'д', 'е', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'ю', 'я', 'і', 'ј', 'ћ', 'ּ', 'א', 'ב', 'ג', 'ו', 'ט', 'י', 'ך', 'ל', 'מ', 'נ', 'ס', 'ע', 'פ', 'ץ', 'צ', 'ק', 'ר', 'ת', 'װ', '،', 'أ', 'إ', 'ا', 'ب', 'ة', 'ت', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'س', 'ش', 'ص', 'ط', 'ع', 'ف', 'ق', 'ل', 'م', 'ن', 'ه', 'و', 'ي', 'پ', 'ک', 'அ', 'ஆ', 'இ', 'க', 'ச', 'ட', 'ண', 'த', 'ந', 'ன', 'ப', 'ம', 'ர', 'ற', 'ல', 'ள', 'ழ', 'வ', 'ா', 'ி', 'ு', 'ே', 'ை', 'ொ', '்', 'ಡ', 'ಮ', 'ರ', 'ಲ', 'ಸ', 'ಿ', 'ก', 'ข', 'ค', 'ง', 'จ', 'ช', 'ฒ', 'ณ', 'ด', 'ต', 'ท', 'ธ', 'น', 'บ', 'ป', 'ผ', 'พ', 'ภ', 'ม', 'ย', 'ร', 'ฤ', 'ล', 'ว', 'ศ', 'ส', 'ห', 'อ', 'ะ', 'ั', 'า', 'ำ', 'ิ', 'ี', 'ื', 'ุ', 'ู', 'เ', 'แ', 'โ', 'ไ', '่', '้', '์', 'ზ', 'უ', 'ფ', 'ḩ', 'ḻ', 'ṟ', 'ṣ', 'ṭ', 'ạ', 'ễ', 'ệ', 'ὶ', '\\u200e', '–', '—', '‘', '’', '“', '”', '†', '‡', '•', '…', '′', '″', '₨', '€', '₱', '℃', 'ℓ', '№', '™', '→', '−', '≠', '♠', '♦', '➔', '\\u3000', 'り', 'ア', 'イ', 'ウ', 'カ', 'ク', 'コ', 'シ', 'ズ', 'ゼ', 'ソ', 'タ', 'チ', 'ッ', 'ツ', 'パ', 'ボ', 'マ', 'ム', 'ャ', 'ョ', 'リ', 'レ', 'ン', 'ー', '一', '三', '上', '下', '东', '中', '主', '义', '九', '乡', '事', '介', '伐', '会', '依', '俳', '僧', '光', '児', '全', '六', '兴', '其', '典', '况', '前', '加', '勇', '務', '化', '华', '単', '卡', '厂', '双', '句', '史', '号', '吉', '吹', '哈', '哪', '国', '國', '地', '坝', '坪', '堡', '大', '姚', '子', '孤', '安', '宗', '家', '寄', '寨', '射', '局', '屋', '岩', '島', '左', '巴', '師', '店', '庙', '延', '建', '得', '心', '怡', '排', '探', '教', '数', '文', '斌', '新', '方', '日', '昌', '明', '春', '昭', '普', '會', '本', '李', '村', '板', '桦', '概', '民', '水', '江', '法', '泥', '泽', '湾', '溪', '潭', '澤', '濟', '無', '燈', '界', '皮', '石', '砲', '磨', '禪', '站', '维', '置', '義', '羹', '育', '胜', '臨', '花', '茜', '莲', '華', '董', '薦', '薩', '虚', '街', '装', '覺', '解', '訪', '語', '话', '语', '赤', '転', '軽', '辞', '農', '达', '逆', '通', '造', '連', '道', '那', '鄉', '里', '野', '録', '镇', '长', '門', '陈', '食', '馬', '马', '鹿', '黄', '김', '준', '태', 'ﬂ', '）', '｜']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9c55U9YRNV5",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title vectorize the text{form-width: \"20%\"}\n",
        "char2index={char: index for index, char in enumerate(vocab)}\n",
        "index2char=np.array(vocab)\n",
        "\n",
        "# print(index2char)\n",
        "# print(char2index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "787AAYbKSJoz",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "20dc24b4-09d1-49a4-c1ec-921e65bc1f13"
      },
      "source": [
        "#@title transfor text to int{form-width: \"20%\"}\n",
        "def char_to_index(char):\n",
        "    char_symbol=char.numpy().decode('utf-8')\n",
        "    char_index= char2index[char_symbol] if char_symbol in char2index else char2index['?']\n",
        "    return char_index\n",
        "dataset_chars_int= dataset_chars.map(\n",
        "    lambda char: tf.py_function(func=char_to_index, inp=[char], Tout=tf.int32)\n",
        ")\n",
        "for x in dataset_chars_int.take(2):\n",
        "  print(x.numpy(),index2char[x.numpy()])\n",
        "print(dataset_chars,dataset_chars_int)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "44 J\n",
            "80 o\n",
            "<DatasetV1Adapter shapes: <unknown>, types: tf.string> <DatasetV1Adapter shapes: <unknown>, types: tf.int32>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Vk7yn6FTrSJ",
        "colab_type": "text"
      },
      "source": [
        "# Prepare Training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5n8qHeo_U7_B",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "13c06539-b5d5-4bea-c18c-2c7ad13734c2"
      },
      "source": [
        "#@title create training sequence{form-width: \"20%\"}\n",
        "seq_len =  201#@param {type:\"integer\"}\n",
        "\n",
        "dataset_seqs=dataset_chars_int.batch(seq_len+1,drop_remainder=True)\n",
        "for x in dataset_seqs.take(2):\n",
        "  print(repr(''.join(index2char[x.numpy()])))\n",
        "  #print(''.join(index2char[x]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Joseph Harold Greenberg (May 28, 1915 – May 7, 2001) was an American linguist, known mainly for his work concerning linguistic typology and the genetic classification of languages.\\n\\nLife\\n\\nEarly life and'\n",
            "' education \\n(Main source: Croft 2003)\\n\\nJoseph Greenberg was born on May 28, 1915 to Jewish parents in Brooklyn, New York. His first great interest was music. At the age of 14, he gave a piano concert in'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg4kXy2vVbKY",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title make pair sequence{form-width: \"20%\"}\n",
        "def split_input_target(chunk):\n",
        "  input_text=chunk[:-1]\n",
        "  target_text=chunk[1:]\n",
        "  return input_text, target_text\n",
        "dataset_pairs=dataset_seqs.map(split_input_target)\n",
        "# for inp,out in dataset_pairs.take(1):\n",
        "#   print(''.join(index2char[inp]))\n",
        "#   print(''.join(index2char[out]))\n",
        "\n",
        "#  oseph GreenbergPauline DonaldaList of German football%\n",
        "#  ---------------------------------------------------------\n",
        "#  Joseph GreenbergPauline DonaldaList of German football"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc6dhqHLXu0g",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title make batches on sequence pairs {form-width: \"20%\"}\n",
        "batch_size=64#@param {type:\"integer\"}\n",
        "# Buffer size to shuffle the dataset (TF data is designed to work\n",
        "# with possibly infinite sequences, so it doesn't attempt to shuffle\n",
        "# the entire sequence in memory. Instead, it maintains a buffer in\n",
        "# which it shuffles elements).\n",
        "buffer_size=100#@param {type:\"integer\"}\n",
        "prefetch_size=10 #@param {type:\"integer\"}\n",
        "# How many items to prefetch before the next iteration.\n",
        "\n",
        "dataset_seqs_batches = dataset_pairs\\\n",
        "                      .shuffle(buffer_size)\\\n",
        "                      .batch(batch_size,drop_remainder=True)\\\n",
        "                      .prefetch(prefetch_size)\n",
        "#ds=dataset_seqs_batches.as_numpy_iterator()                  \n",
        "#print(next(ds))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vfTG9bxcoDf",
        "colab_type": "text"
      },
      "source": [
        "# Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtSeSvpVa6YS",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title build sequential model {form-width: \"20%\"}\n",
        "#embedding dimension\n",
        "#[1,20,1,1] dim=4  Apple -> [1,0,1,1] [...] [...] [...] [1,2,0,1]\n",
        "vocab_size=len(vocab) \n",
        "embedding_dim =  256#@param {type:\"integer\"}\n",
        "rnn_units =  1024#@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.models.Sequential()\n",
        "  # Embedding layer\n",
        "  model.add(tf.keras.layers.Embedding(\n",
        "      input_dim=vocab_size,\n",
        "      output_dim=embedding_dim,\n",
        "      batch_input_shape=[batch_size, None]  \n",
        "      #         time:0123              embedding layer \n",
        "      #batch=3  seq=[I have a apple]       |   [1.2.3.0] [] [] [] [...]\n",
        "      #         seq=[cat eats apple]       |   [11.1.3.0] [] [] [] [...]\n",
        "      #         seq=[god hate apple]       |   [0.2.3.1] [] [] [] [...]\n",
        "\n",
        "  # LSTM layer  //RNN LSTM GRU\n",
        "  ))\n",
        "  model.add(tf.keras.layers.LSTM(\n",
        "      units=rnn_units, #dimensionality of the output space.       \n",
        "                                       \n",
        "      return_sequences=True,            #A|p  p|p p|l   Appl -> pple\n",
        "      stateful=True,\n",
        "      recurrent_initializer=tf.keras.initializers.GlorotNormal()\n",
        "\n",
        "  ))\n",
        "\n",
        "                                                   #embedding layer LSTM   Dense\n",
        "  model.add(tf.keras.layers.Dense(vocab_size))   #A     |   [1,2,3,1]   |     |   p     \n",
        "  return model\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoY2kZWmnF-P",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "1a4aee0b-6f33-465b-a817-3c65f61ca87c"
      },
      "source": [
        " #@title show model {form-width: \"20%\"}\n",
        " model=build_model(vocab_size, embedding_dim, rnn_units, batch_size)\n",
        " model.summary()\n",
        "#  tf.keras.utils.plot_model(\n",
        "#     model,\n",
        "#     show_shapes=True,\n",
        "#     show_layer_names=True,\n",
        "# )"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           158976    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 621)           636525    \n",
            "=================================================================\n",
            "Total params: 6,042,477\n",
            "Trainable params: 6,042,477\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8cgJLkornIV",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title loss function & optimizer {form-width: \"20%\"}\n",
        "learning_rate = 0.001 #@param {type:\"number\"}\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "model.compile(optimizer=optimizer, loss=loss)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-RRrT-pQAMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_dir(learning_rate,seq_len,epochs):\n",
        "    model_id= '_'.join([str(learning_rate),str(seq_len),str(epochs)])\n",
        "    log_dir=os.path.join('logs',model_id)\n",
        "    ckpt_dir= os.path.join('train',model_id)\n",
        "    ckpt_prefix = os.path.join(ckpt_dir, 'ckpt_{epoch}')\n",
        "    \n",
        " \n",
        "    return log_dir, ckpt_dir, ckpt_prefix "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULGfsBW1ujrv",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a3937bac-4617-4d26-d9d2-f386f09f8667"
      },
      "source": [
        "#@title checkpoints and tarining {form-width: \"20%\"}\n",
        "epochs =  150#@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "steps_per_epoch=10#@param {type:\"number\"}\n",
        "log_dir, ckpt_dir, ckpt_prefix = get_dir(learning_rate,seq_len,epochs)\n",
        "\n",
        "print(ckpt_dir,ckpt_prefix )\n",
        "\n",
        "checkpoint_callback=[\n",
        "                     tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1),\n",
        "                     tf.keras.callbacks.ModelCheckpoint(\n",
        "                         filepath=ckpt_dir,\n",
        "                         save_weights_only=True,\n",
        "                        #  mode='min',mointor='val_loss', save_best_only=True,verbose=1\n",
        "                      ),\n",
        "                     #tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5),\n",
        "                     ]\n",
        "\n",
        "ds=dataset_seqs_batches.repeat()\n",
        "#model.load_weights((ckpt_dir))\n",
        "\n",
        "# history = model.fit(\n",
        "#     x=ds.as_numpy_iterator(),\n",
        "#     epochs=epochs,\n",
        "#     steps_per_epoch=steps_per_epoch,\n",
        "#     callbacks=[checkpoint_callback]  \n",
        "# )"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train/0.001_201_150 train/0.001_201_150/ckpt_{epoch}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOg_XFu4egZJ",
        "colab_type": "text"
      },
      "source": [
        "# Generate Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdlHWlynIcdE",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title generate text{form-width: \"20%\"}\n",
        "\n",
        "def generate_text(model, start_string, num_generate = 1000, temperature=1.0):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing).\n",
        "    input_indices = [char2index[s] for s in start_string]\n",
        "    input_indices = tf.expand_dims(input_indices, 0)\n",
        "\n",
        "    # Empty string to store our results.\n",
        "    text_generated = []\n",
        "\n",
        "    # Here batch size == 1.\n",
        "    model.reset_states()\n",
        "    for char_index in range(num_generate):\n",
        "        predictions = model(input_indices)\n",
        "        # remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # Using a categorical distribution to predict the character returned by the model.\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(\n",
        "        predictions,\n",
        "        num_samples=1\n",
        "        )[-1,0].numpy()\n",
        "\n",
        "        # We pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state.\n",
        "        input_indices = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(index2char[predicted_id])\n",
        "\n",
        "    return (start_string +  ''.join(text_generated))\n",
        "    "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gkq0p_IelhU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6fc025e9-2bb2-45ec-8484-17d08988c38b"
      },
      "source": [
        "#@title load checkpoints{form-width: \"20%\"}\n",
        "simplified_batch_size = 1\n",
        "\n",
        "pModel=build_model(vocab_size, embedding_dim, rnn_units,batch_size=1)\n",
        "pModel.load_weights(tf.train.latest_checkpoint(ckpt_dir))\n",
        "#pModel.load_weights(ckpt_dir)\n",
        "pModel.build(tf.TensorShape([simplified_batch_size,None]))\n",
        "\n",
        "print(ckpt_dir,tf.train.latest_checkpoint(ckpt_dir))\n",
        "\n",
        "# Apple eat by cat\n",
        "# People afraid of COVID19\n",
        "# ....."
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train/0.001_201_150 train/0.001_201_150/ckpt_150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K24rrRLIr6i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "92d8ec47-46d5-4a3f-d7ce-930e8558f9e0"
      },
      "source": [
        "#@title generate wiki{form-width: \"20%\"}\n",
        "num_generate=1000 #@param {type:\"integer\"}\n",
        "temporature=0.6 #@param {type:\"number\"}\n",
        "start_string='Deep learning is '\n",
        "\n",
        "generated_text=generate_text(pModel,start_string,num_generate,temporature)\n",
        "\n",
        "\n",
        "import textwrap\n",
        "for x in textwrap.wrap(generated_text,130,tabsize=0):\n",
        "  print(x)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deep learning is a songs in the book to Same became to the current positions in the City of Town of August 1965, and John Wan\n",
            "International Airportence Comeack. In the reating and Candles (state of the most be sports to Sachonore Englands and 1934.\n",
            "Category:Townschose People Championship in 1281 south of British and Kensal elective space to his boad after 1960 and the entire\n",
            "of the night propital of announced in 1973  In ANA However State (Club Was)  2000 (1996)  List of the County French Championship\n",
            "(1972) (Jert (1986) is a state to the Beit posted by competitions of list in 1987, while the inited in 1350 has been home and\n",
            "leading the released a north the member of Arence Finall Martinian from North University of Berships in Musquinian and American to\n",
            "Pendrian Award Afthe County Martinian dames family unere of Spanging simily. The List of Cases and Quiterary 2019.  References\n",
            "External links Category:American links Category:American program in July 2012 (C  - 3:20 Rusel of American Series begind Eri\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1lRVlgCDzkZT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "ade6b9b8-1934-40be-d5c3-7deec9658a47"
      },
      "source": [
        "#@title generate wiki{form-width: \"20%\"}\n",
        "num_generate=1000 #@param {type:\"integer\"}\n",
        "temporature=1.0 #@param {type:\"number\"}\n",
        "start_string='Deep learning is '\n",
        "\n",
        "generated_text=generate_text(pModel,start_string,num_generate,temporature)\n",
        "\n",
        "\n",
        "import textwrap\n",
        "for x in textwrap.wrap(generated_text,130,tabsize=0):\n",
        "  print(x)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deep learning is with Agenvational pade of Bue Madzezch. Hotil Massachum, he 7 was fiving aghist and allwdays in a vale for bound.\n",
            "It was from behin 2012 slores of Lock Kanuka, In Goot) As Schose Massilon Aditivist in Effroull Enger monercy and therqume to\n",
            "villed in diffienchion culture apporated, with see gome Institute Azericand, ascart and proatterage crafing on Jandical\n",
            "Anterlingte Elmings A Misilia finature to mater and Intluditist Film-70-in Internam daying Sing, International Matherington also\n",
            "poirf.  The nughnes distributingured to competition and 1/coadde. It was ex stratesition. on 30 Othpart 2iven also Qee Diseu daugn\n",
            "and the regit totes preticiplid.  West coust severia  Onoughts of Glence Dause Désorationa (Micha Ertrica inlignto M136) was\n",
            "matteriation of Nuvrall Airporied College Dury 1978  Forewn Depus A=\"RadKE838 2000) and clinct, a hist and pici of the rine toost\n",
            "kax also reacing, this who were with a varaction louncis-a grish finary Film 1995.2 18, Stom Such house  of April 2025) metav\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}